{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "task5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "Tnfi18T5pP0s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a634613-ce3a-4857-f793-544ab7bb692c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ],
      "source": [
        "import torch\n",
        "import sklearn.datasets\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "from typing import Iterator, List, Callable, Tuple\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlretrieve\n",
        "urlretrieve('https://raw.githubusercontent.com/LawrenceDuan/IMDb-Review-Analysis/master/IMDb_Reviews.csv', 'IMDB_Dataset.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89pT4KSlpzq0",
        "outputId": "8d9f8526-9e31-4e29-8dc0-088c9f6a7c38"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('IMDB_Dataset.csv', <http.client.HTTPMessage at 0x7f84df7df090>)"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('IMDB_Dataset.csv')\n",
        "data_reviews, data_labels = data.review, data.sentiment.tolist()"
      ],
      "metadata": {
        "id": "vqorZi5cp_5U"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, validation_data, train_labels, validation_labels = train_test_split(data_reviews, data_labels, test_size=0.1, random_state=42)\n",
        "train_data, test_data, train_labels, test_labels = train_test_split(data_reviews, data_labels, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "_Sg6RzJrrmaf"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_to_words(data):\n",
        "  reviews = []\n",
        "  for review in data:\n",
        "    review_cleaned = nltk.tokenize.word_tokenize(review)\n",
        "    reviews.append(review_cleaned)\n",
        "\n",
        "  return reviews\n",
        "\n",
        "train_reviews = transform_to_words(train_data)\n",
        "validation_reviews = transform_to_words(validation_data)\n",
        "test_reviews = transform_to_words(test_data)"
      ],
      "metadata": {
        "id": "9Ad_mLe21FlR"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vocab(data):\n",
        "  words = set([word for review in data for word in review])\n",
        "  return words\n",
        "\n",
        "train_vocab = get_vocab(train_reviews)\n",
        "val_vocab = get_vocab(validation_reviews)\n",
        "test_vocab = get_vocab(test_data)"
      ],
      "metadata": {
        "id": "GMp5Z0eA6LoE"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import operator\n",
        "def word_freq(data, min_aparitions):\n",
        "    \n",
        "    all_words = [words.lower() for sentences in data for words in sentences]\n",
        "    sorted_vocab = sorted(dict(Counter(all_words)).items(), key=operator.itemgetter(1))\n",
        "    final_vocab = [k for k,v in sorted_vocab if v > min_aparitions]\n",
        "\n",
        "    return final_vocab\n",
        "\n",
        "train_vocab = word_freq(train_reviews, min_aparitions = 10)\n",
        "val_vocab = word_freq(validation_reviews, min_aparitions = 10)\n",
        "test_vocab = word_freq(test_reviews, min_aparitions = 10)"
      ],
      "metadata": {
        "id": "0o8Hres98z-O"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGhfgX4TMF2q",
        "outputId": "c48aed8e-e7ed-4bcd-cec6-6f8f739f946a"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "27223\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_word_indices = dict((c, i + 2) for i, c in enumerate(train_vocab))\n",
        "train_indices_word = dict((i + 2, c) for i, c in enumerate(train_vocab))\n",
        "\n",
        "train_indices_word[0] = 'UNK'\n",
        "train_word_indices['UNK'] = 0\n",
        "\n",
        "train_indices_word[1] = 'PAD'\n",
        "train_word_indices['PAD'] = 1\n",
        "\n",
        "\n",
        "val_word_indices = dict((c, i + 2) for i, c in enumerate(val_vocab))\n",
        "val_indices_word = dict((i + 2, c) for i, c in enumerate(val_vocab))\n",
        "\n",
        "val_indices_word[0] = 'UNK'\n",
        "val_word_indices['UNK'] = 0\n",
        "\n",
        "val_indices_word[1] = 'PAD'\n",
        "val_word_indices['PAD'] = 1\n",
        "\n",
        "test_word_indices = dict((c, i + 2) for i, c in enumerate(test_vocab))\n",
        "test_indices_word = dict((i + 2, c) for i, c in enumerate(test_vocab))\n",
        "\n",
        "test_indices_word[0] = 'UNK'\n",
        "test_word_indices['UNK'] = 0\n",
        "\n",
        "test_indices_word[1] = 'PAD'\n",
        "test_word_indices['PAD'] = 1"
      ],
      "metadata": {
        "id": "nkWKYONX-CvY"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_sentences(data, char_indices, one_hot = False):\n",
        "    vectorized = []\n",
        "    for sentences in data:\n",
        "\n",
        "        # transformam fiecare review in reprezentarea lui sub forma de indici ale caracterelor continute\n",
        "        sentences_of_indices = [char_indices[w] if w in char_indices.keys() else char_indices['UNK'] for w in sentences]\n",
        "\n",
        "        # pentru fiecare indice putem face reprezentarea one-hot corespunzatoare\n",
        "        # sau putem sa nu facem asta si sa adaugam un embedding layer in model care face această transformare\n",
        "        if one_hot:\n",
        "            sentences_of_indices = np.eye(len(char_indices))[sentences_of_indices]\n",
        "\n",
        "        vectorized.append(sentences_of_indices)\n",
        "\n",
        "    return vectorized"
      ],
      "metadata": {
        "id": "PJ4Uw8Uo-ZOq"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad(samples, max_length):\n",
        "    \n",
        "    return torch.tensor([\n",
        "        sample[:max_length] + [1] * max(0, max_length - len(sample))\n",
        "        for sample in samples\n",
        "    ])"
      ],
      "metadata": {
        "id": "svVBROY5-maa"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_reviews_vectorized = vectorize_sentences(train_reviews, train_word_indices)\n",
        "val_reviews_vectorized = vectorize_sentences(validation_reviews, val_word_indices)\n",
        "test_reviews_vectorized = vectorize_sentences(test_reviews, test_word_indices)\n",
        "\n",
        "train_reviews_vectorized = pad(train_reviews_vectorized, max_length = 512)\n",
        "val_reviews_vectorized = pad(val_reviews_vectorized, max_length = 512)\n",
        "test_reviews_vectorized = pad(test_reviews_vectorized, max_length = 512)"
      ],
      "metadata": {
        "id": "7h0XdOWJ-ECJ"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_reviews_vectorized.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfOOYcoUJiA1",
        "outputId": "523a5de3-8538-409d-fcc4-b4272bddc1ae"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([45000, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ReviewDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "            \n",
        "    def __getitem__(self, k):\n",
        "        \"\"\"Returneaza al k-lea exemplu din dataset\"\"\"\n",
        "        return self.data[k], self.labels[k]\n",
        "    \n",
        "    def __len__(self):\n",
        "        \"\"\"Returneaza dimensiunea datasetului\"\"\"\n",
        "        return len(self.data)"
      ],
      "metadata": {
        "id": "chRuUzuU0W5t"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = ReviewDataset(data=train_reviews_vectorized, labels=train_labels)\n",
        "validation_dataset = ReviewDataset(data=val_reviews_vectorized, labels=validation_labels)\n",
        "test_dataset = ReviewDataset(data=test_reviews_vectorized, labels=test_labels)"
      ],
      "metadata": {
        "id": "u2xvgVAY9dAN"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
        "validation_dataloader = DataLoader(dataset=validation_dataset, batch_size=64, shuffle=True)\n",
        "test_dataloader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "ONUPKvnI9pXh"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Definim un embedding layer cu un vocabular de dimensiune 72\n",
        "        # și ca output un embedding de dimensiune 20\n",
        "        # padding_idx este indexul din vocabular al paddingului (1, în cazul nostru)\n",
        "        \n",
        "        self.embedding = torch.nn.Embedding(27225, 100, padding_idx=1)\n",
        "\n",
        "        self.dropout = torch.nn.Dropout(0.4)\n",
        "        \n",
        "        conv1 = torch.nn.Sequential(\n",
        "            torch.nn.Conv1d(in_channels=100, out_channels=128, kernel_size=3, padding=1),\n",
        "            torch.nn.BatchNorm1d(128),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool1d(kernel_size=2),\n",
        "        )\n",
        "        \n",
        "        conv2 = torch.nn.Sequential(\n",
        "            torch.nn.Conv1d(in_channels=128, out_channels=128, kernel_size=5, padding=2),\n",
        "            torch.nn.BatchNorm1d(128),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool1d(kernel_size=2),\n",
        "        )\n",
        "\n",
        "        conv3 = torch.nn.Sequential(\n",
        "            torch.nn.Conv1d(in_channels=128, out_channels=128, kernel_size=5, padding=2),\n",
        "            torch.nn.BatchNorm1d(128),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool1d(kernel_size=2)\n",
        "        )\n",
        "        \n",
        "        global_average = torch.nn.AvgPool1d(kernel_size=64, stride=64)\n",
        "\n",
        "        self.convolutions = torch.nn.Sequential(\n",
        "            conv1, conv2, conv3, global_average\n",
        "        )\n",
        "        \n",
        "        self.flatten = torch.nn.Flatten()\n",
        "        self.output = torch.nn.Linear(in_features=128, out_features=2)\n",
        "\n",
        "        \n",
        "    def forward(self, input):\n",
        "        # trecem inputul prin layerul de embedding\n",
        "        embeddings = self.embedding(input)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        \n",
        "        # permutăm inputul astfel încât prima dimensiune este numărul de channels\n",
        "        embeddings = embeddings.permute(0, 2, 1)\n",
        "        \n",
        "        # trecem inputul prin secvența de layere\n",
        "        out = self.convolutions(embeddings)\n",
        "        out = self.output(self.flatten(out))\n",
        "        return out"
      ],
      "metadata": {
        "id": "DgJJVQW-1Sz4"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, train_dataloader, loss_crt, optimizer):\n",
        "    \"\"\"\n",
        "    model: Model object \n",
        "    train_dataloader: DataLoader over the training dataset\n",
        "    loss_crt: loss function object\n",
        "    optimizer: Optimizer object\n",
        "\n",
        "    The function returns: \n",
        "     - the epoch training loss, which is an average over the individual batch\n",
        "       losses\n",
        "     - the predictions made by the model\n",
        "     - the labels \n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    num_batches = len(train_dataloader)\n",
        "    predictions = []\n",
        "    labels = []\n",
        "    for idx, batch in tqdm(enumerate(train_dataloader)):\n",
        "        batch_data, batch_labels = batch\n",
        "\n",
        "        output = model(batch_data)\n",
        "        batch_predictions = torch.argmax(output, dim=1)\n",
        "        \n",
        "        predictions += batch_predictions.tolist()\n",
        "        labels += batch_labels.squeeze().tolist()\n",
        "\n",
        "        loss = loss_crt(output, batch_labels)\n",
        "        loss_scalar = loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        model.zero_grad()\n",
        "\n",
        "        epoch_loss += loss_scalar\n",
        "\n",
        "    epoch_loss = epoch_loss/num_batches\n",
        "\n",
        "    return epoch_loss, predictions, labels\n",
        "\n",
        "def eval_epoch(model, val_dataloader, loss_crt):\n",
        "    \"\"\"\n",
        "    model: Model object \n",
        "    val_dataloader: DataLoader over the validation dataset\n",
        "    loss_crt: loss function object\n",
        "\n",
        "    The function returns: \n",
        "     - the epoch validation loss, which is an average over the individual batch\n",
        "       losses\n",
        "     - the predictions made by the model\n",
        "     - the labels \n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    epoch_loss = 0.0\n",
        "    num_batches = len(val_dataloader)\n",
        "    predictions = []\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in tqdm(enumerate(val_dataloader)):\n",
        "            batch_data, batch_labels = batch\n",
        "            \n",
        "            output = model(batch_data)\n",
        "            batch_predictions = torch.argmax(output, dim=1)\n",
        "        \n",
        "            predictions += batch_predictions.tolist()\n",
        "            labels += batch_labels.squeeze().tolist()\n",
        "\n",
        "            loss = loss_crt(output, batch_labels)\n",
        "            loss_scalar = loss.item()\n",
        "\n",
        "            epoch_loss += loss_scalar\n",
        "\n",
        "    epoch_loss = epoch_loss/num_batches\n",
        "\n",
        "    return epoch_loss, predictions, labels"
      ],
      "metadata": {
        "id": "tbGI9UnX7Nik"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_accuracy(predictions: List[int], labels:List[int]) -> float:\n",
        "    \"\"\"\n",
        "    Compute accuracy given the predictions of a binary classifier and the \n",
        "    ground truth label.\n",
        "    predictions: list of model predictions (0 or 1)\n",
        "    labels: list of ground truth labels (0 or 1)\n",
        "    \"\"\"\n",
        "    num_correct = len([(p,l) for (p,l) in zip(predictions,labels) if p==l])\n",
        "    epoch_accuracy = num_correct/len(labels)\n",
        "    \n",
        "    return epoch_accuracy"
      ],
      "metadata": {
        "id": "oOe9Ftvc7T-G"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model()\n",
        "\n",
        "loss_criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n",
        "\n",
        "train_losses, val_losses = [], []\n",
        "train_accuracies, val_accuracies = [], []\n",
        "\n",
        "for epoch_idx in range(10):\n",
        "    train_epoch_loss, train_predictions, train_labels = train_epoch(\n",
        "        model, \n",
        "        train_dataloader, \n",
        "        loss_criterion, \n",
        "        optimizer\n",
        "    )\n",
        "    val_epoch_loss, val_predictions, val_labels = eval_epoch(\n",
        "        model,\n",
        "        validation_dataloader,\n",
        "        loss_criterion,\n",
        "    )\n",
        "    train_acc = compute_accuracy(train_predictions, train_labels)\n",
        "    val_acc = compute_accuracy(val_predictions, val_labels)\n",
        "    train_losses.append(train_epoch_loss)\n",
        "    val_losses.append(val_epoch_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "    val_accuracies.append(val_acc)\n",
        "\n",
        "    print(\"epoch %d, train loss=%f, train acc=%f, val loss=%f, val acc=%f\" % (\n",
        "        epoch_idx, \n",
        "        train_epoch_loss,\n",
        "        train_acc,\n",
        "        val_epoch_loss,\n",
        "        val_acc\n",
        "    ))"
      ],
      "metadata": {
        "id": "J1J1dpNf7WzR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "945c08fa-cef7-4789-8573-e9ce0681f4d7"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "704it [08:54,  1.32it/s]\n",
            "79it [00:14,  5.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, train loss=0.490831, train acc=0.760667, val loss=0.814999, val acc=0.498400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "704it [08:53,  1.32it/s]\n",
            "79it [00:14,  5.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1, train loss=0.339225, train acc=0.853533, val loss=0.983205, val acc=0.479600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "704it [08:44,  1.34it/s]\n",
            "79it [00:14,  5.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2, train loss=0.286281, train acc=0.882889, val loss=0.919215, val acc=0.496600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "704it [08:51,  1.33it/s]\n",
            "79it [00:14,  5.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 3, train loss=0.256295, train acc=0.896156, val loss=0.946038, val acc=0.500200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "704it [08:44,  1.34it/s]\n",
            "79it [00:14,  5.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 4, train loss=0.233078, train acc=0.906800, val loss=1.014972, val acc=0.505200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "704it [08:49,  1.33it/s]\n",
            "79it [00:14,  5.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 5, train loss=0.212461, train acc=0.915822, val loss=1.077705, val acc=0.502200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "704it [08:46,  1.34it/s]\n",
            "79it [00:14,  5.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 6, train loss=0.197603, train acc=0.922378, val loss=1.187248, val acc=0.501400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "704it [08:41,  1.35it/s]\n",
            "79it [00:14,  5.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 7, train loss=0.187115, train acc=0.927022, val loss=1.451706, val acc=0.490600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "704it [08:43,  1.35it/s]\n",
            "79it [00:14,  5.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 8, train loss=0.174274, train acc=0.931667, val loss=2.235968, val acc=0.498600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "704it [08:48,  1.33it/s]\n",
            "79it [00:14,  5.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 9, train loss=0.157846, train acc=0.937956, val loss=1.284705, val acc=0.494800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_losses = []\n",
        "test_accuracies = []\n",
        "\n",
        "test_loss, test_predictions, test_labels = eval_epoch(model, test_dataloader, loss_criterion)\n",
        "test_acc = compute_accuracy(test_predictions, test_labels)\n",
        "test_losses.append(test_loss)\n",
        "test_accuracies.append(test_acc)\n",
        "\n",
        "print(\"test loss=%f, test acc=%f\" % (test_loss, val_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdKziqo3w_O6",
        "outputId": "687c794b-394f-4a5b-f0b8-7bf4692d6ff0"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "79it [00:14,  5.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test loss=1.277900, test acc=0.494800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}