{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "task9.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A27GtH3QvHQl",
        "outputId": "afc06828-ea29-4c39-ed4b-c18465ea9d3f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "\n",
        "root_path = 'gdrive/MyDrive/Colab Notebooks/lab9-nlp/'\n",
        "with open(root_path+'intents.json') as json_data:\n",
        "    intents = json.load(json_data)"
      ],
      "metadata": {
        "id": "UfAvi3Jtvyga"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(intents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c1c6ikmy1-e",
        "outputId": "8dbb95a7-f533-4fcd-9e29-7bca7becf59c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'intents': [{'tag': 'greeting', 'patterns': ['Hi', 'How are you', \"What's up?\", 'Hello', 'Good day'], 'responses': ['Hello, how are you today?', 'Good to see you again. How are you?', 'Hi there, how are you?'], 'context_set': 'howareyou'}, {'tag': 'greeting_good', 'patterns': [\"I'm fine\", \"I'm pretty good\", \"I'm happy today\"], 'responses': [\"I'm very happy to hear that!\", \"Yay, thats's great news!\"], 'context_filter': 'howareyou'}, {'tag': 'greeting_bad', 'patterns': ['Not so good', \"I'm not feeling myself today\", 'I am sadder than ever.'], 'responses': ['Oh, I know jokes to make you feel better.', 'Do you want me to tell you a joke to feel better?'], 'context_filter': 'howareyou'}, {'tag': 'joke', 'patterns': ['ok, tell me a joke', 'tell me a joke', 'do you know any jokes'], 'responses': ['What did the wall tell to the other? See you at the corner!', \"When Chuck Norris cuts an onion, he doesn't cry, the onion cries!\"]}, {'tag': 'goodbye', 'patterns': ['Bye', 'See you later', 'Goodbye'], 'responses': ['See you later', 'Have a nice day', 'Bye! Come back again soon.']}, {'tag': 'thanks', 'patterns': ['Thanks', 'Thank you', \"That's helpful\"], 'responses': ['Happy to help!', 'Any time!', 'My pleasure']}, {'tag': 'funny', 'patterns': ['hahaha', 'that was funny', ':))'], 'responses': ['Glad I made you smile', 'Happy to make you smile']}, {'tag': 'quote', 'patterns': ['Can you tell me a quote to brighten up my day?', 'Tell me a quote.'], 'responses': [\"Nobody got anywhere in the world by simply being content. Louis L'Amour (1908 - 1988)\", 'Write a wise saying and your name will live forever. Anonymous']}, {'tag': 'name', 'patterns': ['What is your name?', \"What's your name?\", 'Who are you?'], 'responses': ['I am Chatty, your beloved chatbot.']}, {'tag': 'therapy', 'patterns': ['What kind of therapy do you do?', \"How come you're a therapy chatbot?\"], 'responses': ['The main way to help you is through talk. More like the journaling technique, what happens is that you will feel lighter after ranting your problems to me.']}, {'tag': 'real_therapy', 'patterns': ['Can I talk to a real therapist?', 'I need to talk to a real therapist', 'I want to talk to a human being'], 'responses': ['Here is a list of specialists that can help you: https://static.anaf.ro/static/10/Anaf/posturi_vacante/Lista_furnizori_servicii_psihologice_autorizate.pdf']}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iv2WN_U_t0f_",
        "outputId": "d0ef256c-9aff-451e-bddc-37b475f01637"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tflearn\n",
            "  Downloading tflearn-0.5.0.tar.gz (107 kB)\n",
            "\u001b[?25l\r\u001b[K     |███                             | 10 kB 22.9 MB/s eta 0:00:01\r\u001b[K     |██████                          | 20 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 30 kB 16.3 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 40 kB 11.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 51 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 61 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 71 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 81 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 92 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 102 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 107 kB 7.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tflearn) (1.21.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tflearn) (1.15.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from tflearn) (7.1.2)\n",
            "Building wheels for collected packages: tflearn\n",
            "  Building wheel for tflearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tflearn: filename=tflearn-0.5.0-py3-none-any.whl size=127299 sha256=a981d53437cd8290506a99a8fe15882c3579bed14c561575efb58b2d76ef1b0d\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/14/2e/1d8e28cc47a5a931a2fb82438c9e37ef9246cc6a3774520271\n",
            "Successfully built tflearn\n",
            "Installing collected packages: tflearn\n",
            "Successfully installed tflearn-0.5.0\n"
          ]
        }
      ],
      "source": [
        "! pip install tflearn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "stemmer = LancasterStemmer()\n",
        "\n",
        "import numpy as np\n",
        "import tflearn\n",
        "import tensorflow as tf\n",
        "import random"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZH7Y-LlKuCUo",
        "outputId": "fa0a1c75-76d4-4d13-b506-0f1d9bc139fe"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = []\n",
        "classes = []\n",
        "documents = []\n",
        "ignore_words = ['?',\"'m\", \"'re\", \"'s\", ')', ',', '.', ':']  # eventual aici putem defini o lista de STOPWORDS\n",
        "\n",
        "# pentru fiecare fraza din sabloanele corespunzatoare unei intentii\n",
        "for intent in intents['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "        # tokenizare\n",
        "        w = nltk.word_tokenize(pattern)\n",
        "        # adaugam cuvintele la o lista globala cu toate cuvintele din texte\n",
        "        words.extend(w)\n",
        "        # adaugam textul tokenizat la multimea de documente\n",
        "        documents.append((w, intent['tag']))\n",
        "        # adaugam clasa (tipul de intentie) la lista de clase existente\n",
        "        if intent['tag'] not in classes:\n",
        "            classes.append(intent['tag'])\n",
        "\n",
        "# aplicam stemming si lowercasing pentru fiecare cuvant intalnit, ignoram stopwords\n",
        "words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n",
        "words = sorted(list(set(words)))  # eliminam duplciate\n",
        "\n",
        "classes = sorted(list(set(classes)))\n",
        "\n",
        "print (len(documents), \"documents\", documents[0], documents[7])\n",
        "print (len(classes), \"classes\", classes)\n",
        "print (len(words), \"unique stemmed words\", words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXP7K8zXuPuh",
        "outputId": "71c6beaa-2103-4e6b-8379-a76a2af80431"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33 documents (['Hi'], 'greeting') (['I', \"'m\", 'happy', 'today'], 'greeting_good')\n",
            "11 classes ['funny', 'goodbye', 'greeting', 'greeting_bad', 'greeting_good', 'joke', 'name', 'quote', 'real_therapy', 'thanks', 'therapy']\n",
            "61 unique stemmed words ['a', 'am', 'any', 'ar', 'being', 'bright', 'bye', 'can', 'chatbot', 'com', 'day', 'do', 'ev', 'feel', 'fin', 'funny', 'good', 'goodby', 'hahah', 'happy', 'hello', 'help', 'hi', 'how', 'hum', 'i', 'is', 'jok', 'kind', 'know', 'lat', 'me', 'my', 'myself', 'nam', 'nee', 'not', 'of', 'ok', 'pretty', 'quot', 'real', 'sad', 'see', 'so', 'talk', 'tel', 'than', 'thank', 'that', 'therap', 'therapy', 'to', 'today', 'up', 'want', 'was', 'what', 'who', 'yo', 'you']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# construim datele de antrenare\n",
        "training = []\n",
        "output = []\n",
        "# un vector de 0 de lungime egala cu numarul de clase (vom folosi reprezentarea one-hot a claselor)\n",
        "output_empty = [0] * len(classes)\n",
        "\n",
        "# bag of words pentru fiecare fraza\n",
        "for doc in documents:\n",
        "    # initializam bag of words\n",
        "    bag = []\n",
        "    # lista de tokeni pentru fraza curenta\n",
        "    pattern_words = doc[0]\n",
        "    # aplicam stemming\n",
        "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
        "    # construim vectorul binar pentru bag of words\n",
        "    for w in words:\n",
        "        bag.append(1) if w in pattern_words else bag.append(0)\n",
        "\n",
        "    # output este '1' pentru tagul corespunzator frazei si '0' pentru celelalte (one-hot)\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(doc[1])] = 1\n",
        "\n",
        "    training.append([bag, output_row])\n",
        "\n",
        "print(training[1])\n",
        "print(type(training))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9XxD2IpuSdu",
        "outputId": "97e6d2e1-e714-49a2-9276-f769404f91b1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# shuffle + transformare in np.array\n",
        "random.shuffle(training)\n",
        "training = np.array(training)\n",
        "\n",
        "print(training.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zom3bBkduVKW",
        "outputId": "c4387fdf-6072-43d4-a1a2-3d9a64a5f3f8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(33, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# feature-urile de antrenare\n",
        "train_x = list(training[:,0])\n",
        "# etichete/labels (ce dorim sa returneze modelul)\n",
        "train_y = list(training[:,1])"
      ],
      "metadata": {
        "id": "rChy4zqRuhuq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.framework import ops\n",
        "ops.reset_default_graph()  # resetam starea engine-ului TensorFlow\n",
        "\n",
        "# definim reteaua\n",
        "net = tflearn.input_data(shape=[None, len(train_x[0])])  # toti vectorii de bag-of-words au aceasta dimensiune\n",
        "net = tflearn.fully_connected(net, 8) # strat feed-forward ascuns cu 8 noduri\n",
        "net = tflearn.fully_connected(net, 8) # strat feed-forward ascuns cu 8 noduri\n",
        "net = tflearn.fully_connected(net, len(train_y[0]), activation='softmax') # numarul de noduri output = numarul de clase\n",
        "net = tflearn.regression(net)  # folosim acest strat de logistic regression pentru a extrage probabilitatile claselor\n",
        "\n",
        "# definim modelul final\n",
        "model = tflearn.DNN(net, tensorboard_dir='tflearn_logs')\n",
        "\n",
        "# incepem antrenarea folosind coborarea pe gradient\n",
        "# trecem prin model cate 8 fraze odata (batch size = 8)\n",
        "model.fit(train_x, train_y, n_epoch=1000, batch_size=8, show_metric=True)\n",
        "model.save('model.tflearn')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9puKfM2ukGU",
        "outputId": "9176813f-83c3-4ad2-9b4b-acda78fd9002"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Step: 4999  | total loss: \u001b[1m\u001b[32m0.06953\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 1000 | loss: 0.06953 - acc: 0.9976 -- iter: 32/33\n",
            "Training Step: 5000  | total loss: \u001b[1m\u001b[32m0.06759\u001b[0m\u001b[0m | time: 0.034s\n",
            "| Adam | epoch: 1000 | loss: 0.06759 - acc: 0.9978 -- iter: 33/33\n",
            "--\n",
            "INFO:tensorflow:/content/model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "pickle.dump( {'words':words, 'classes':classes, 'train_x':train_x, 'train_y':train_y}, open( \"training_data\", \"wb\" ) )"
      ],
      "metadata": {
        "id": "82Z4hSTj1yTX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pickle.load( open( \"training_data\", \"rb\" ) )\n",
        "print(type(data))\n",
        "\n",
        "for key in data.keys():\n",
        "    print(key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIJ4v7s5117a",
        "outputId": "bbd575c0-8423-4c25-c0eb-51e145f2d68b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'dict'>\n",
            "words\n",
            "classes\n",
            "train_x\n",
            "train_y\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = data['words']\n",
        "classes = data['classes']\n",
        "train_x = data['train_x']\n",
        "train_y = data['train_y']"
      ],
      "metadata": {
        "id": "GYsDjShW150s"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(root_path+'intents.json') as json_data:\n",
        "    intents = json.load(json_data)"
      ],
      "metadata": {
        "id": "LYikxJ8t18Pk"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load('model.tflearn')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7wkAzHz1-ea",
        "outputId": "690cb643-e363-4ffa-cba0-dab81fe5baa5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/model.tflearn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_up_sentence(sentence):\n",
        "    # tokenizare\n",
        "    sentence_words = nltk.word_tokenize(sentence)\n",
        "    # stemming\n",
        "    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
        "    return sentence_words\n",
        "\n",
        "# reprezentare binara bag-of-words\n",
        "def bow(sentence, words):\n",
        "    sentence_words = clean_up_sentence(sentence)\n",
        "    # bag of words\n",
        "    bag = [0]*len(words)  \n",
        "    for s in sentence_words:\n",
        "        for i,w in enumerate(words):\n",
        "            if w == s: \n",
        "                bag[i] = 1\n",
        "\n",
        "    return(np.array(bag))"
      ],
      "metadata": {
        "id": "FqVY_F1V2Bd9"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ERROR_THRESHOLD = 0.25\n",
        "def classify(sentence):\n",
        "    # probabilitatile prezise de model\n",
        "    results = model.predict([bow(sentence, words)])[0]\n",
        "    # renuntam la intentiile cu probabilitate mica\n",
        "    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD]\n",
        "    # sortam descrescator intentiile dupa probabilitate\n",
        "    results.sort(key=lambda x: x[1], reverse=True)\n",
        "    return_list = []\n",
        "    for r in results:\n",
        "        return_list.append((classes[r[0]], r[1]))\n",
        "    # return perechi (intentie, probabilitate)\n",
        "    return return_list"
      ],
      "metadata": {
        "id": "EW0DK_NP2G69"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = {}\n",
        "\n",
        "def response(sentence, userID='123', show_details=False):\n",
        "    results = classify(sentence)\n",
        "    # daca avem macar o intentie valida, o procesam pe cea cu probabilitate maxima\n",
        "    if results:\n",
        "        # loop as long as there are matches to process\n",
        "        while results:\n",
        "            for i in intents['intents']:\n",
        "                # cautam in dictionarul de intentii tagul returnat\n",
        "                if i['tag'] == results[0][0]:\n",
        "                    # daca intentia curenta asteapta un context pentru a fi valida, verificam ca contextul actual sa fie indeplinit\n",
        "                    if (\n",
        "                        not 'context_filter' in i\n",
        "                        or (userID in context and 'context_filter' in i and i['context_filter'] == context[userID])\n",
        "                    ):\n",
        "                        if show_details:\n",
        "                            print('tag:', i['tag'])\n",
        "                        # daca intentia curenta actualizeaza contextul\n",
        "                        if 'context_set' in i:\n",
        "                            if show_details:\n",
        "                                print('context:', i['context_set'])\n",
        "                            context[userID] = i['context_set']\n",
        "                        # returnam un raspuns aleator corespunzator intentiei\n",
        "                        return random.choice(i['responses'])\n",
        "            results.pop(0) # daca nu am putut da un raspuns pentru aceasta intentie, trecem la urmatoarea cu probabilitate maxima\n",
        "\n",
        "    return \"Sorry, I wasn't trained about this subject.\"  # nu a putut fi stabilita o intentie pentru fraza introdusa\n",
        "\n",
        "print(response(\"hello there\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2A9sJObc2JRb",
        "outputId": "b7aa7ac4-feba-4b04-8eb0-3177ae0acfc0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Good to see you again. How are you?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    while True:\n",
        "        print(response(input(\"user: \")))\n",
        "except KeyboardInterrupt:\n",
        "    pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4Dc1DBA2KyM",
        "outputId": "b897d3ad-7c2e-4b62-f2f7-b98967b46e17"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user: Hi!\n",
            "Good to see you again. How are you?\n",
            "user: Fine, you?\n",
            "Have a nice day\n",
            "user: Thanks!\n",
            "Happy to help!\n"
          ]
        }
      ]
    }
  ]
}